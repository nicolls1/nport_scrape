# -*- coding: utf-8 -*-
import scrapy
from scrapy.http import Request

from python_scraper.items import CompanyItem


class SecNPORTPSpider(scrapy.Spider):
    # Unique spider name
    name = 'sec_nport_p'

    # Parse URLS as long as they're from the 'sec.report' domain
    allowed_domains = ['sec.report']

    # Start requests here
    start_url = 'https://sec.report/Document/Header/?formType=NPORT-P&keyword=corporate'

    # Parse through page
    max_pages = 15

    # Called by Scrapy when spider begins
    # Provide Request information for the Scrapy Downloader
    def start_requests(self):
        for page in range(1, self.max_pages + 1):
            yield Request(f'{self.start_url}&page={page}', callback=self.parse)

    # Lambda method passed into Request value in start_requests method
    # Run by Scrapy Downloader
    #
    # Response holds nport search page information generated by Scrapy Downloader
    def parse(self, response):
        # Parse response in XPath format
        for url in response.xpath('//table//tr/td[2]/a/@href').extract():
            yield response.follow(f'{url}primary_doc.xml', callback=self.parse_xml, headers={
                'Accept': 'application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en',
            })

    # Lambda method added to Scrapy parsing queue
    # Called in the parse lambda
    #
    # Response contains parsed XML nport info
    def parse_xml(self, response):
        response.selector.register_namespace('x', 'http://www.sec.gov/edgar/nport')
        return CompanyItem(
            series_lei=response.xpath('//x:seriesLei/text()').extract()[0],
            name=response.xpath('//x:regName/text()').extract()[0],
            series_name=response.xpath('//x:seriesName/text()').extract()[0],
            rep_pd_date=response.xpath('//x:repPdDate/text()').extract()[0],
            total_assets=response.xpath('//x:totAssets/text()').extract()[0],
            total_liabilities=response.xpath('//x:totLiabs/text()').extract()[0],
            net_assets=response.xpath('//x:netAssets/text()').extract()[0],
        )
